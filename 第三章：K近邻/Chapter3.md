<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# 第三章：K近邻
- 模型要素：
  - 训练集：k近邻是非参数模型，对示例的分类依赖训练集本身
  - 距离度量：如何衡量实例间的距离
    - $L_p$距离或Minkowski距离
    $$
    L_p(x_i, xj)=\Big(\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p\Big)^{\frac{1}{p}}
    $$
    - 绝对值距离即$L_1$距离，欧式距离即$L_2$距离
  - k值
    - 如果选择的K值较小，则近似误差（训练误差）会减小，但估计误差（校验误差）会增大，K值越小，过拟合风险越高。
    - K=N时，无需训练，每次都会返回训练集中频数最高的类
  - 分类规则
    - 一般依据多数表决规则，这样经验误差最小。假设误分类的概率为$P(Y\ne f(X))=1-P(Y=f(X))$，则对于一个$x\in X$，其最近的k个训练实例构成的集合$N_k(x)$。如果涵盖$N_k(X)$的区域类别是$c_j$，那么误分类概率是：
    $$
    \frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i\ne c_j)=1-\frac{1}{k}\sum_{x_i\in N_k(x)}I(y_i=c_j)
    $$
    要使经验风险最小，则使$\sum_{x_i\in N_k(x)}I(y_i=c_j)$最大，所以多数表决等价于经验风险最小。
- 模型实现
实现k近邻算法如果采用线性扫描的方法，每次都计算新实例与训练集所有实例的距离，会导致计算量过大。其实在训练集很大时，真正决定测试实例结果的只占训练集所有实例的一小部分，我们要做的其实是需要快速的找到这一小部分的实例，再去计算他们与测试实例间的相互距离就行了。kd树就是这样一种存储训练数据的特殊数据结构。
  - 构造kd树
  kd树其实是一个二叉树，从根节点开始，每次将剩余实例2分。构造方法如下：
    - 构造根节点
    - 生成子节点，即选择k维空间里的一个坐标轴$l$ ($l=j(mod\,k)+1$, $j$是节点的深度)，并在坐标轴上找一个切分点，用以确定一个垂直于坐标轴的超平面，可以将父节点的实例分为两个部分，即两个子节点。
    - 重复生成子节点的过程，直到分出的子节点内没有实例为止。    

    如果每次都以实例在选定坐标轴上的中位数（median）为切分点，则会构造出平衡的kd树。
  - 搜索kd树（最近邻）
    1. 搜索包含目标点x的叶节点，从根节点出发，递归的向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到做子节点，否则移动到右子节点，知道子节点为叶节点为止。
    2. 以此叶节点为【当前最近点】
    3. 递归的向上回退，在每个节点进行以下操作    
        - 如果该节点比【当前最近点】离目标点更近，则更新该实例点为【当前最近点】
        - 当前最近点一定存在于该节点的一个子节点对应的区域。检查该子节点的父节点的另一子节点对应的区域是否有更近的点。具体的，检查另一子节点对应的区域是否与以目标点为球心，以目标点与【当前最近点】间的距离为半径的超球体相交。如果相交，可能在另一个子节点对应的区域内存在距目标点更近的点，移动到另一个子节点，继续递归向下搜索最近邻，如不相交，则向上回退。
    4. 当回退到根节点时搜索结束，此时保存的【当前最近点】即为最近邻。
  